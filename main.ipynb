{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - 190349K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "### Requirements\n",
    "\n",
    "-   Python libraries (install with `pip install numpy pandas matplotlib seaborn sklearn xgboost`)\n",
    "    -   numpy\n",
    "    -   pandas\n",
    "    -   matplotlib\n",
    "    -   seaborn\n",
    "    -   sklearn\n",
    "    -   xgboost\n",
    "-   Datasets\n",
    "    -   Should be inside `data/` directory\n",
    "    -   `data/train.csv`\n",
    "    -   `data/valid.csv`\n",
    "    -   `data/test.csv`\n",
    "\n",
    "### Saving and loading models\n",
    "\n",
    "-   Models that are trained are also saved to `models/` in the `joblib` format\n",
    "-   `models/label_1_before` is the model for label 1 before feature engineering, and `models/label_1_after` is the one after feature engineering\n",
    "-   Calls to the `load_model` function have been commented out (running this notebook as is will train each model for the first time)\n",
    "-   To reuse the saved models, find the calls to the `save_model` function and comment that line plus the line before it (that trains the model)\n",
    "-   Then uncomment the calls to the `load_model` function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Constants\n",
    "L1 = 'label_1'\n",
    "L2 = 'label_2'\n",
    "L3 = 'label_3'\n",
    "L4 = 'label_4'\n",
    "LABELS = [L1, L2, L3, L4]\n",
    "AGE_LABEL = L2\n",
    "FEATURES = [f'feature_{i}' for i in range(1, 257)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[LABELS + [FEATURES[i] for i in range(0, 256, 32)]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- `RobustScaler` is used to scale the features\n",
    "- For age (label_2), rows where label is missing are filtered out\n",
    "- For accent (label_4), unequal distribution is handled when training the model later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# To store datasets for each label\n",
    "X_train = {}\n",
    "X_valid = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_valid = {}\n",
    "y_test = {}\n",
    "y_pred_before = {}\n",
    "y_pred_after = {}\n",
    "\n",
    "\n",
    "def filter_missing_age(df: pd.DataFrame):\n",
    "    \"\"\"Filter out rows where age is `NaN`\"\"\"\n",
    "    return df[df[AGE_LABEL].notna()]\n",
    "\n",
    "\n",
    "# Filter `NaN` and scale datasets\n",
    "for target_label in LABELS:\n",
    "    tr_df = filter_missing_age(train_df) if target_label == AGE_LABEL else train_df\n",
    "    vl_df = filter_missing_age(valid_df) if target_label == AGE_LABEL else valid_df\n",
    "    ts_df = test_df  # No need to filter rows with missing age in test dataset\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train[target_label] = pd.DataFrame(\n",
    "        scaler.fit_transform(tr_df.drop(LABELS, axis=1)), columns=FEATURES\n",
    "    )\n",
    "    y_train[target_label] = tr_df[target_label]\n",
    "    X_valid[target_label] = pd.DataFrame(\n",
    "        scaler.transform(vl_df.drop(LABELS, axis=1)), columns=FEATURES\n",
    "    )\n",
    "    y_valid[target_label] = vl_df[target_label]\n",
    "    X_test[target_label] = pd.DataFrame(\n",
    "        scaler.transform(ts_df.drop(LABELS, axis=1)), columns=FEATURES\n",
    "    )\n",
    "    y_test[target_label] = ts_df[target_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[L1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels and showing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nans(y_true: pd.Series, y_pred: pd.Series):\n",
    "    \"\"\"Filter `NaN`s in both `y_true` and `y_pred` based on `NaN`s in `y_true`\"\"\"\n",
    "    return y_true[y_true.isna() == False], y_pred[y_true.isna() == False]\n",
    "\n",
    "\n",
    "def predict(model, X_test: pd.DataFrame, y_test: pd.Series, categorical=True):\n",
    "    y_pred: pd.Series = model.predict(X_test)\n",
    "    print(\"Stats:\")\n",
    "    if categorical:\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, y_pred))\n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", metrics.precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "        print(\"Recall:\", metrics.recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "    else:\n",
    "        print(\n",
    "            \"RMSE:\",\n",
    "            metrics.mean_squared_error(*filter_nans(y_test, y_pred), squared=False),\n",
    "        )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "\n",
    "def save_model(model, name: str):\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "    joblib.dump(model, f\"{MODEL_DIR}/{name}.joblib\")\n",
    "\n",
    "\n",
    "def load_model(name: str):\n",
    "    return joblib.load(f\"{MODEL_DIR}/{name}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def train_xgboost_binary(X_train: pd.DataFrame, y_train: pd.Series):\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "def train_xgboost_regression(X_train: pd.DataFrame, y_train: pd.Series):\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def train_svm(X_train: pd.DataFrame, y_train: pd.Series, balance=False, categorical=True):\n",
    "    if categorical:\n",
    "        if balance:\n",
    "            clf = svm.SVC(kernel=\"linear\", class_weight=\"balanced\")\n",
    "        else:\n",
    "            clf = svm.SVC(kernel=\"linear\")\n",
    "    else:\n",
    "        clf = svm.SVR(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_svm(X_train[L1], y_train[L1])  # To use the pre-saved model, comment out this line and the next one\n",
    "save_model(model, \"label_1_before\")\n",
    "# model = load_model(\"label_1_before\")  # Then uncomment this line to load that pre-saved model\n",
    "y_pred_before[L1] = predict(model, X_test[L1], y_test[L1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_xgboost_regression(X_train[L2], y_train[L2])\n",
    "save_model(model, \"label_2_before\")\n",
    "# model = load_model(\"label_2_before\")\n",
    "y_pred_before[L2] = predict(model, X_test[L2], y_test[L2], categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_svm(X_train[L3], y_train[L3])\n",
    "save_model(model, \"label_3_before\")\n",
    "# model = load_model(\"label_3_before\")\n",
    "y_pred_before[L3] = predict(model, X_test[L3], y_test[L3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_svm(X_train[L4], y_train[L4], balance=True)\n",
    "save_model(model, \"label_4_before\")\n",
    "# model = load_model(\"label_4_before\")\n",
    "y_pred_before[L4] = predict(model, X_test[L4], y_test[L4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "-   Methods used\n",
    "    -   Principal component analysis (PCA)\n",
    "    -   Recursive feature elimination (attempted and dropped afterwards)\n",
    "    -   Univariate feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif, r_regression, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    SVC = \"SVC\"\n",
    "    XGB = \"XGBoost regressor\"\n",
    "    RANDOM_FOREST = \"Random forest\"\n",
    "    LINEAR = \"Linear regressor\"\n",
    "\n",
    "\n",
    "def fit_and_transform_pca(X_train: pd.DataFrame, X_test: pd.DataFrame):\n",
    "    pca = PCA(n_components=0.95, svd_solver=\"full\")\n",
    "    pca.fit(X_train)\n",
    "    X_train_trf = pd.DataFrame(pca.transform(X_train))\n",
    "    X_test_trf = pd.DataFrame(pca.transform(X_test))\n",
    "    print(\"Shape after PCA:\", X_train_trf.shape)\n",
    "    return X_train_trf, X_test_trf\n",
    "\n",
    "\n",
    "# Not used as it is time-consuming\n",
    "def transform_with_rfe(X: pd.DataFrame, y: pd.Series):\n",
    "    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    print(\"Shape after RFE:\", X.shape)\n",
    "    return rfe, rfe.transform(X)\n",
    "\n",
    "\n",
    "def univariate_feature_selection(\n",
    "    X: pd.DataFrame, y: pd.Series, categorical=True, feature_count=30\n",
    "):\n",
    "    if categorical:\n",
    "        score_func = f_classif\n",
    "    else:\n",
    "        score_func = r_regression\n",
    "    selector = SelectKBest(score_func, k=feature_count)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    print(\"Shape after univariate:\", X_new.shape)\n",
    "    return selector, X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from typing import Dict\n",
    "\n",
    "# Test datasets after transforming for each label\n",
    "X_test_transformed: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def transform_train_predict(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    target_label: str,\n",
    "    categorical=True,\n",
    "    model_type: Model = Model.SVC,\n",
    "    feature_count=30,\n",
    "    pca_count=5\n",
    "):\n",
    "    X_train_trf, X_test_trf = fit_and_transform_pca(X_train, X_test)\n",
    "\n",
    "    # # Recursive feature elimination is commented out as it is very time consuming\n",
    "    # rfe, X_train_trf = transform_with_rfe(X_train, y_train)\n",
    "    # X_test_trf = rfe.transform(X_test_trf)\n",
    "\n",
    "    # Skip univariate feature selection if `feature_count` is specified as 0\n",
    "    if feature_count != 0:\n",
    "        selector, X_train_trf = univariate_feature_selection(\n",
    "            X_train_trf, y_train, categorical=categorical, feature_count=feature_count\n",
    "        )\n",
    "        X_test_trf = pd.DataFrame(selector.transform(X_test_trf))\n",
    "    \n",
    "    # Re-run PCA multiple times\n",
    "    for _ in range(pca_count):\n",
    "        X_train_trf, X_test_trf = fit_and_transform_pca(X_train_trf, X_test_trf)\n",
    "    # X_train_trf, X_test_trf = fit_and_transform_pca(X_train_trf, X_test_trf)\n",
    "    print(\"Model:\", model_type)\n",
    "\n",
    "    # Training model\n",
    "    if categorical or model_type == Model.SVC:\n",
    "        model = svm.SVC(kernel=\"rbf\", class_weight=\"balanced\")\n",
    "    elif model_type == Model.XGB:\n",
    "        model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    elif model_type == Model.RANDOM_FOREST:\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    elif model_type == Model.LINEAR:\n",
    "        model = LinearRegression()\n",
    "    model.fit(X_train_trf, y_train)\n",
    "    save_model(model, f\"{target_label}_after\")\n",
    "\n",
    "    # model = load_model(f\"{target_label}_after\")\n",
    "    y_pred = predict(model, X_test_trf, y_test, categorical=categorical)\n",
    "    return y_pred, X_test_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after[L1], X_test_transformed[L1] = transform_train_predict(\n",
    "    X_train[L1], y_train[L1], X_test[L1], y_test[L1], L1, feature_count=0, pca_count=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after[L2], X_test_transformed[L2] = transform_train_predict(\n",
    "    X_train[L2],\n",
    "    y_train[L2],\n",
    "    X_test[L2],\n",
    "    y_test[L2],\n",
    "    L2,\n",
    "    categorical=False,\n",
    "    model_type=Model.XGB,\n",
    "    feature_count=0,\n",
    "    pca_count=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after[L3], X_test_transformed[L3] = transform_train_predict(\n",
    "    X_train[L3], y_train[L3], X_test[L3], y_test[L3], L3, feature_count=0, pca_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after[L4], X_test_transformed[L4] = transform_train_predict(\n",
    "    X_train[L4], y_train[L4], X_test[L4], y_test[L4], L4, feature_count=0, pca_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_COLS_FIRST = [\n",
    "    \"Predicted labels before feature engineering\",\n",
    "    \"Predicted labels after feature engineering\",\n",
    "    \"No of new features\",\n",
    "]\n",
    "OUT_COLS_FEATURES = [f\"new_feature_{i}\" for i in range(1, 257)]\n",
    "OUT_COLS = OUT_COLS_FIRST + OUT_COLS_FEATURES\n",
    "\n",
    "\n",
    "def save_results_to_csv(label: str, no_of_features: int, X_test: pd.DataFrame):\n",
    "    df = pd.DataFrame([], columns=OUT_COLS)\n",
    "    df[OUT_COLS[0]] = y_pred_before[label]\n",
    "    df[OUT_COLS[1]] = y_pred_after[label]\n",
    "    col2 = np.empty(len(y_pred_before[label]))\n",
    "    col2.fill(no_of_features)\n",
    "    df[OUT_COLS[2]] = col2.astype(int)\n",
    "    for i in range(1, no_of_features+1):\n",
    "        df[f\"new_feature_{i}\"] = X_test[i - 1]\n",
    "    empty_col = np.empty(len(y_pred_before[label]))\n",
    "    empty_col.fill(0)\n",
    "    for i in range(no_of_features+1, 257):\n",
    "        df[f\"new_feature_{i}\"] = empty_col.astype(int)\n",
    "    df.to_csv(f\"results/190349K_{label}.csv\", index=False)\n",
    "\n",
    "\n",
    "save_results_to_csv(L1, len(X_test_transformed[L1].columns), X_test_transformed[L1])\n",
    "save_results_to_csv(L2, len(X_test_transformed[L2].columns), X_test_transformed[L2])\n",
    "save_results_to_csv(L3, len(X_test_transformed[L3].columns), X_test_transformed[L3])\n",
    "save_results_to_csv(L4, len(X_test_transformed[L4].columns), X_test_transformed[L4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlation between features\n",
    "\n",
    "- This shows that there is no longer any significant correlation between the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_correlation(target_label: str):\n",
    "    correlation_matrix = X_test_transformed[target_label].corr()\n",
    "    correlation_threshold = 0.5\n",
    "\n",
    "    filtered_correlation_matrix = correlation_matrix[\n",
    "        (correlation_matrix > correlation_threshold) | (correlation_matrix < -correlation_threshold)\n",
    "    ]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(filtered_correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title(f\"Correlation Heatmap (Filtered) for {target_label.replace('_', ' ')}\")\n",
    "    plt.savefig(f\"plots/correlation_{target_label}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(L4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
